{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "installation (sans GPU) via :  \n",
    "pip install --upgrade mxnet  \n",
    "pip install autogluon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autogluon as ag\n",
    "import pandas as pd\n",
    "from autogluon import TabularPrediction as task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On récupère le dataset Titanic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loaded data from: ../datasources/titanic/train.csv | Columns = 12 / 12 | Rows = 891 -> 891\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PassengerId  Survived  Pclass  \\\n",
      "0            1         0       3   \n",
      "1            2         1       1   \n",
      "2            3         1       3   \n",
      "3            4         1       1   \n",
      "4            5         0       3   \n",
      "\n",
      "                                                Name     Sex   Age  SibSp  \\\n",
      "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
      "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
      "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
      "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
      "4                           Allen, Mr. William Henry    male  35.0      0   \n",
      "\n",
      "   Parch            Ticket     Fare Cabin Embarked  \n",
      "0      0         A/5 21171   7.2500   NaN        S  \n",
      "1      0          PC 17599  71.2833   C85        C  \n",
      "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
      "3      0            113803  53.1000  C123        S  \n",
      "4      0            373450   8.0500   NaN        S  \n"
     ]
    }
   ],
   "source": [
    "train_data = task.Dataset(file_path=\"../datasources/titanic/train.csv\")\n",
    "print(train_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Détail sur la colonne survivant: \n",
      " count    891.000000\n",
      "mean       0.383838\n",
      "std        0.486592\n",
      "min        0.000000\n",
      "25%        0.000000\n",
      "50%        0.000000\n",
      "75%        1.000000\n",
      "max        1.000000\n",
      "Name: Survived, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Détail sur la colonne survivant: \\n\", train_data['Survived'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrainons le modèle sur les données du Titanic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to models/\n",
      "Train Data Rows:    891\n",
      "Train Data Columns: 12\n",
      "Preprocessing data ...\n",
      "Here are the first 10 unique label values in your data:  [0 1]\n",
      "AutoGluon infers your prediction problem is: binary  (because only two unique label-values observed)\n",
      "If this is wrong, please specify `problem_type` argument in fit() instead (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Feature Generator processed 891 data points with 33 features\n",
      "Original Features:\n",
      "\tint features: 4\n",
      "\tobject features: 5\n",
      "\tfloat features: 2\n",
      "Generated Features:\n",
      "\tint features: 22\n",
      "All Features:\n",
      "\tint features: 26\n",
      "\tobject features: 5\n",
      "\tfloat features: 2\n",
      "\tData preprocessing and feature engineering runtime = 0.34s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: accuracy\n",
      "To change this, specify the eval_metric argument of fit()\n",
      "AutoGluon will early stop models using evaluation metric: accuracy\n",
      "/opt/anaconda3/lib/python3.7/imp.py:342: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  return _load(spec)\n",
      "Fitting model: RandomForestClassifierGini ...\n",
      "\t0.8268\t = Validation accuracy score\n",
      "\t0.81s\t = Training runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: RandomForestClassifierEntr ...\n",
      "\t0.8045\t = Validation accuracy score\n",
      "\t0.71s\t = Training runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: ExtraTreesClassifierGini ...\n",
      "\t0.7877\t = Validation accuracy score\n",
      "\t0.6s\t = Training runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: ExtraTreesClassifierEntr ...\n",
      "\t0.7933\t = Validation accuracy score\n",
      "\t0.62s\t = Training runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: KNeighborsClassifierUnif ...\n",
      "\t0.6089\t = Validation accuracy score\n",
      "\t0.01s\t = Training runtime\n",
      "\t0.11s\t = Validation runtime\n",
      "Fitting model: KNeighborsClassifierDist ...\n",
      "\t0.6145\t = Validation accuracy score\n",
      "\t0.01s\t = Training runtime\n",
      "\t0.11s\t = Validation runtime\n",
      "Fitting model: LightGBMClassifier ...\n",
      "\t0.8268\t = Validation accuracy score\n",
      "\t0.3s\t = Training runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatboostClassifier ...\n",
      "\t0.8156\t = Validation accuracy score\n",
      "\t0.89s\t = Training runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetClassifier ...\n",
      "\t0.838\t = Validation accuracy score\n",
      "\t6.0s\t = Training runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: LightGBMClassifierCustom ...\n",
      "\t0.8268\t = Validation accuracy score\n",
      "\t0.52s\t = Training runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: weighted_ensemble_k0_l1 ...\n",
      "\t0.8547\t = Validation accuracy score\n",
      "\t0.39s\t = Training runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 13.3s ...\n"
     ]
    }
   ],
   "source": [
    "dir = 'models'\n",
    "label_col = 'Survived'\n",
    "predictor = task.fit(train_data=train_data, label=label_col, output_directory=dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: accuracy on test data: 0.961841\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"accuracy\": 0.9618406285072951,\n",
      "    \"accuracy_score\": 0.9618406285072951,\n",
      "    \"balanced_accuracy_score\": 0.954702329594478,\n",
      "    \"matthews_corrcoef\": 0.919374313148082,\n",
      "    \"f1_score\": 0.9618406285072951\n",
      "}\n",
      "Detailed (per-class) classification report:\n",
      "{\n",
      "    \"0\": {\n",
      "        \"precision\": 0.9541446208112875,\n",
      "        \"recall\": 0.9854280510018215,\n",
      "        \"f1-score\": 0.9695340501792115,\n",
      "        \"support\": 549\n",
      "    },\n",
      "    \"1\": {\n",
      "        \"precision\": 0.9753086419753086,\n",
      "        \"recall\": 0.9239766081871345,\n",
      "        \"f1-score\": 0.9489489489489489,\n",
      "        \"support\": 342\n",
      "    },\n",
      "    \"accuracy\": 0.9618406285072951,\n",
      "    \"macro avg\": {\n",
      "        \"precision\": 0.9647266313932981,\n",
      "        \"recall\": 0.954702329594478,\n",
      "        \"f1-score\": 0.9592414995640801,\n",
      "        \"support\": 891\n",
      "    },\n",
      "    \"weighted avg\": {\n",
      "        \"precision\": 0.9622681844904067,\n",
      "        \"recall\": 0.9618406285072951,\n",
      "        \"f1-score\": 0.9616326981918379,\n",
      "        \"support\": 891\n",
      "    }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions:   [0 1 1 1 0 0 0 0 1 1 1 1 0 0 0 1 0 0 0 1 0 1 1 0 0 1 0 0 1 0 0 1 1 0 0 0 0\n",
      " 0 0 1 0 0 0 1 1 0 0 1 0 0 0 0 1 1 0 0 1 0 1 0 0 1 0 0 0 0 1 0 1 0 0 0 0 0\n",
      " 1 0 0 0 1 1 0 1 1 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 1 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1\n",
      " 0 1 1 0 0 1 0 1 1 1 1 0 0 1 1 0 0 0 0 1 0 0 1 1 1 0 1 0 0 0 1 1 0 1 0 0 0\n",
      " 0 0 1 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 1 1 1 1\n",
      " 1 0 1 0 0 0 0 0 1 1 1 0 0 1 0 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 1 0 0 0 0\n",
      " 0 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 0 0 1 1 0 1 1 0 0 1 1 0 1 0 1 1 1 1 0 0\n",
      " 0 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 1 0 0 0 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1\n",
      " 1 0 0 0 0 1 1 0 0 0 1 1 0 1 0 0 0 1 0 1 1 1 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 1 0 1 0 1 1 0 0 0 0 0 0 0 0 1 1 0 1 1 1 1 0 0 1 0 1 0 0 1 0 0 1\n",
      " 0 1 1 0 1 1 0 0 0 1 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 1 0 0 1 1 0 0 0 0 0 1 0\n",
      " 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 0 1 1 0 1 1 0 1 1 0 0 1 0\n",
      " 1 0 1 0 0 1 0 0 1 0 0 0 1 0 0 1 0 1 0 1 0 1 1 0 0 1 0 0 1 1 0 1 1 0 0 0 1\n",
      " 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 1 1 0 0 1 1 1 0 1 1 0 0 0 1 0 1 0 0 0 1\n",
      " 0 0 0 0 1 0 0 1 1 0 0 0 1 0 0 0 1 1 0 0 1 0 0 1 0 0 1 0 0 1 1 0 0 0 0 1 0\n",
      " 0 1 0 1 0 0 1 0 0 0 0 0 1 0 1 1 1 0 1 0 1 0 1 0 1 0 0 0 0 0 0 1 0 0 0 1 0\n",
      " 0 0 0 1 1 0 0 1 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 0\n",
      " 0 0 0 1 1 1 1 1 0 0 0 0 0 1 1 0 0 1 0 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0 1 0 0\n",
      " 1 0 1 0 0 0 0 1 0 0 1 1 0 0 1 1 0 0 0 1 0 0 1 1 0 1 0 0 0 0 0 0 0 0 1 0 0\n",
      " 1 0 1 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 1 0 0 0 1 1 1 0 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 0 0 0 1 0 0 1 0 0 1 1 1 1 1 1 0 0 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0\n",
      " 0 0 1 0 1 1 1 1 0 0 0 1 0 0 1 1 0 0 1 0 1 0 0 1 1 0 0 0 1 1 0 0 0 0 0 0 1\n",
      " 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "predictor = task.load(dir) # Nécéssaire seulement si le modèle n'avait pas été chargé au préalable\n",
    "y_train = train_data[label_col]\n",
    "x_train_data = train_data.drop(labels=[label_col],axis=1) \n",
    "y_train_pred = predictor.predict(x_train_data)\n",
    "\n",
    "print(\"Predictions:  \", y_train_pred)\n",
    "perf = predictor.evaluate_predictions(y_true=y_train, y_pred=y_train_pred, auxiliary_metrics=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regardons avec les données de test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test_data = task.Dataset(file_path='../datasources/titanic/test.csv')\n",
    "print(test_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions:   [0 1 0 0 1 0 1 0 1 0 0 0 1 0 1 1 0 0 0 1 0 1 1 1 1 0 1 0 0 0 0 0 1 1 1 0 1\n",
      " 1 0 1 0 0 0 1 1 0 0 0 1 1 0 0 1 1 0 0 0 0 0 1 0 0 0 0 1 1 1 0 0 1 1 0 0 0\n",
      " 1 0 0 1 0 1 0 0 0 0 0 0 1 0 1 1 1 0 1 0 0 0 1 0 1 0 1 0 0 0 1 0 0 0 0 0 0\n",
      " 1 1 1 1 0 0 1 0 1 1 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0\n",
      " 0 0 1 0 0 1 0 0 1 1 0 1 1 1 1 0 0 0 0 0 1 0 0 0 0 0 0 1 1 1 1 1 0 1 1 0 1\n",
      " 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 1 1 1 0 0 1 0 1 0 0 0 0 1 0 0 1 0 1 0 1 0\n",
      " 1 0 1 1 0 1 0 0 0 1 0 0 0 0 0 0 1 1 1 1 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 0 1\n",
      " 0 0 0 1 1 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 1 1 1 1 0 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 1 1 0 0 1 0 0\n",
      " 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 1 0 1 0 0 0 0 1 1 0 1 0 0 0 1 0\n",
      " 0 1 0 0 1 1 1 0 0 0 0 0 1 1 0 1 0 0 0 0 0 1 1 0 1 1 0 1 0 0 1 0 1 0 0 0 0\n",
      " 0 1 1 1 1 0 0 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "predictor = task.load(dir) # Nécéssaire seulement si le modèle n'avait pas été chargé au préalable\n",
    "y_pred = predictor.predict(test_data)\n",
    "print(\"Predictions:  \", y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>892</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>893</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>894</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>895</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>896</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  PassengerId Survived\n",
       "0         892        0\n",
       "1         893        1\n",
       "2         894        1\n",
       "3         895        1\n",
       "4         896        0"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final = pd.DataFrame()\n",
    "for i in range(len(test_data)):\n",
    "    row = {'PassengerId' : str(test_data['PassengerId'][i]) , 'Survived' : str(y_pred[i])}\n",
    "    final = final.append(row , ignore_index=True)\n",
    "final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.to_csv(\"result.csv\", columns=[\"PassengerId\", \"Survived\"], index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ouch ! après soumission sur kaggle on obtient un score de 0.55980 ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici, nous discutons de ce qui s'est passé pendant fit ().\n",
    "\n",
    "Puisqu'il n'y a que deux valeurs possibles de la variable de classe, il s'agissait d'un problème de classification binaire, pour lequel une mesure de performance appropriée est la précision. AutoGluon déduit automatiquement cela ainsi que le type de chaque entité (c'est-à-dire quelles colonnes contiennent des nombres continus par rapport à des catégories discrètes). AutogGluon peut également gérer automatiquement les problèmes courants tels que les données manquantes et la mise à l'échelle des valeurs des fonctionnalités.\n",
    "\n",
    "Nous n'avons pas spécifié de données de validation distinctes et donc AutoGluon choisit automatiquement une division aléatoire de formation / validation des données. Les données utilisées pour la validation sont séparées des données d'apprentissage et sont utilisées pour déterminer les modèles et les valeurs d'hyperparamètre qui produisent les meilleurs résultats. Plutôt qu'un simple modèle, AutoGluon forme plusieurs modèles et les assemble pour garantir des performances prédictives supérieures.\n",
    "\n",
    "Par défaut, AutoGluon essaie de s'adapter à différents types de modèles, y compris les réseaux de neurones et les ensembles d'arbres. Chaque type de modèle possède différents hyperparamètres que l'utilisateur devrait traditionnellement spécifier. AutoGluon automatise ce processus.\n",
    "\n",
    "AutoGluon teste automatiquement et itérativement les valeurs des hyperparamètres pour produire les meilleures performances sur les données de validation. Cela implique la formation répétée de modèles sous différents paramètres d'hyperparamètres et l'évaluation de leurs performances. Ce processus peut être gourmand en calcul, donc fit () peut paralléliser ce processus sur plusieurs threads (et machines si des ressources distribuées sont disponibles). Pour contrôler les exécutions, vous pouvez spécifier divers arguments dans fit () comme illustré dans le didacticiel In-Depth suivant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
